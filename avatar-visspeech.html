<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <script src="http://www.google.com/jsapi" type="text/javascript"></script>
  <script type="text/javascript">google.load("jquery", "1.3.2");</script>
  <!-- CSS
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="assets/css/normalize.css' %}">
  <link rel="stylesheet" href="assets/css/skeleton.css' %}">
  <link rel="stylesheet" href="assets/css/general.css' %}">
  <link rel="stylesheet" href="assets/css/custom.css' %}">
  <link rel="stylesheet" href="assets/css/styles.css">
  <script type="text/javascript" src="resources/hidebib.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <title>AVATAR: Unconstrained Audiovisual Speech Recognition</title>
    <meta property='og:title' content='AVATAR: Unconstrained Audiovisual Speech Recognition' />
    <meta property="og:description" content="AVATAR: Unconstrained Audiovisual Speech Recognition" />
  </head>
  <body>
    <br>
    </br>
    </br>
    <center>
      <span style="font-size:40px;color:Navy"><b>AVATAR: Unconstrained Audiovisual Speech Recognition</span></b>
      </br>
      </br>
      </br>
      <table align=center width=1000px>
        <tr>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://gabeur.github.io/" target="_blank">Valentin Gabeur*</a></span>
            </center>
          </td>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://phseo.github.io/" target="_blank">Paul Hongsuck Seo*</a></span>
            </center>
          </td>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://a-nagrani.github.io/" target="_blank">Arsha Nagrani*</a></span>
            </center>
          </td>
        </tr>
        <tr>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://chensun.me/" target="_blank">Chen Sun</a></span>
            </center>
          </td>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://lear.inrialpes.fr/people/alahari/" target="_blank">Karteek Alahari</a></span>
            </center>
          </td>
          <td align=center width=150px>
            <center>
              <span style="font-size:20px"><a href="https://www.di.ens.fr/willow/people_webpages/cordelia/" target="_blank">Cordelia Schmid</a></span>
            </center>
          </td>
        </tr>
      </table>
    </center>
    <!-- Primary Page Layout
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <center>
    <div class="container">
    <div class="row">
    <center>
      <div class="twelve columns" style="margin-top: 3%">
      <div class="container">

        </br>
        <div class="row" align="justify">
          <h2 class="section-title"><span>Abstract</span></h2>
          <p>
	  Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR that incorporates visual cues, often from the movements of a speaker's mouth. Unlike works that simply focus on the lip motion, we investigate the contribution of entire visual frames (visual actions, objects, background etc.). This is particularly useful for unconstrained videos, where the speaker is not necessarily visible. To solve this task, we propose a new sequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained end-to-end from spectrograms and full-frame RGB. To prevent the audio stream from dominating training, we propose different word-masking strategies, thereby encouraging our model to pay attention to the visual stream. We demonstrate the contribution of the visual modality on the How2 AV-ASR benchmark, especially in the presence of simulated noise, and show that our model outperforms all other prior work by a large margin. Finally, we also create a new, real-world test bed for AV-ASR called VisSpeech, which demonstrates the contribution of the visual modality under challenging audio conditions.
          </p>
        </div>

        <div class="row" align="justify">
          <h2 class="section-title"><span>Architecture</span></h2>
          <p>
		We propose a Seq2Seq architecture for audio-visual speech recognition. Our model is trained end-to-end from RGB pixels and spectrograms.
          </p>
	      <div class="row" align=center>
		<img width="700" class="center" src="images/architecture_avatar.png">
	      </div>
        </br>
        </div>

        </br>
        <div class="row" align="justify">
          <h2 class="section-title"><span>Qualitative results</span></h2>
          <p>
		Qualitative results on the VisSpeech dataset. We show the ground truth (GT), and predictions from our audio only (A) and audio-visual model (A+V). Note how the visual context helps with objects ("chain", "eggplant", "coin", "dough"), as well as actions ("knit", "fold") which may be ambiguous from the audio stream alone. Errors in the predictions compared to the GT are highlighted in red.
          </p>
	      <div class="row" align=center>
		<img width="1000" class="center" src="images/qualitative_visspeech.png">
	      </div>
        </div>

        <div align="left" class="row section topspace">
          <h2 class="section-title"><span>Resources</span></h2>
          <ul>
            <li>
              The VisSpeech dataset can be downloaded at <a href="data/VisSpeech.zip"> this link</a>.
            </li>
          </ul>
        </div>

        <div align="left" class="row section topspace">
          <h2 class="section-title"><span>Publication</span></h2>
          <div class="ref">
            <div class="authors">
              V. Gabeur,
              P. Hongsuck Seo,
              A. Nagrani,
              C. Sun,
              K. Alahari,
              C. Schmid,
            </div>
            <div class="title">
              <a href="https://arxiv.org/abs/2206.07684">
              AVATAR: Unconstrained Audiovisual Speech Recognition
              </a> &nbsp;
            </div>
            <div class="conf">
              INTERSPEECH, 2022
            </div>
		<a href="https://arxiv.org/abs/2206.07684">arXiv</a> / <a href="bib/Gabeur2022Avatar.txt">bibtex</a>
          </div>
        </div>
      </div>
    </center>
    <br><br>
    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
  </body>
</html>
