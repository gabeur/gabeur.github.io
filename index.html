<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Valentin Gabeur</title>
    <meta name="author" content="Valentin Gabeur">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/valentin_profile_pic.jpg">
  </head>
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:65%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Valentin Gabeur</name>
                  </p>
                  <p>
                  AI researcher at Google DeepMind (Gemini), specializing in multi-modal learning. Postdoc at Meta AI (SAM 2), PhD from Inria, MSc from Toulouse University, MSc from Icam. Former mechanical engineer with 6 years of experience in industrial automation.
		  </p>
                  <p>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:firstname.lastname@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="Valentin_Gabeur_CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=74zyaaYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/gabeur/"> LinkedIn </a> &nbsp/&nbsp
                    <a href="https://twitter.com/vgabeur">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/gabeur"> GitHub </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/valentin_profile_pic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/valentin_profile_pic.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <heading>Research publications:</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">


            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/coq.png" alt="sam2" width="160" height=auto>
                </td>
                <td width="75%" valign="middle">
                  <p>
                  <a href="https://ai.meta.com/blog/segment-anything-2">
                    <papertitle>SAM 2: Segment Anything in Images and Videos</papertitle>
                  </a>
                  <br>
                  Nikhila Ravi*, <b>Valentin Gabeur*</b>, Yuan-Ting Hu*, Ronghang Hu*, Chaitanya Ryali*, Tengyu Ma*, Haitham Khedr*, Roman Rädle*, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan, Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer*
                  <br>
                  <em>arXiv, 2024</em> &nbsp
                  <br>
                  <a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/">paper</a> / <a href="https://sam2.metademolab.com/">demo</a> / <a href="https://ai.meta.com/sam2">project page</a> / <a href="https://ai.meta.com/blog/segment-anything-2">blog</a> / <a href="https://ai.meta.com/datasets/segment-anything-video">dataset</a> / <a href=https://github.com/facebookresearch/segment-anything-2>github</a>
                  <br>
                  Promptable visual segmentation in images and videos.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/avatar.png" alt="avatar" width="160" height=auto>
                </td>
                <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2206.07684">
                    <papertitle>AVATAR: Unconstrained Audiovisual Speech Recognition</papertitle>
                  </a>
                  <br>
                  <b>Valentin Gabeur*</b>, Paul Hongsuck Seo*, Arsha Nagrani*, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>INTERSPEECH, 2022</em> &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2206.07684">arXiv</a> / <a href="https://gabeur.github.io/avatar-visspeech">project page</a> / <a href="bib/Gabeur2022Avatar.txt">bibtex</a>
                  <br>
		  Leveraging the full frame visual context to improve speech recognition in videos.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MMCVR.png" alt="mmcvr" width="160" height=auto>
                </td>
                <td width="75%" valign="middle">
		  <p>
                  <a href="https://arxiv.org/abs/2111.01300">
                    <papertitle>Masking Modalities for Cross-modal Video Retrieval</papertitle>
                  </a>
                  <br>
                  <b>Valentin Gabeur</b>, Arsha Nagrani, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>WACV, 2022</em> &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2111.01300">arXiv</a> / <a href="bib/Gabeur2022Masking.txt">bibtex</a>
                  <br>
                  Pre-training strategy for learning multi-modal fusion from unlabelled videos.
		  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MMT.png" alt="mmt" width="160" height=auto>
                </td>
                <td width="75%" valign="middle">
		  <p>
                  <a href="https://arxiv.org/abs/2007.10639">
                    <papertitle>Multi-modal Transformer for Video Retrieval</papertitle>
                  </a>
                  <br>
                  <b>Valentin Gabeur</b>, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>ECCV, 2020</em> <b>(Spotlight paper)</b> &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/2007.10639">arXiv</a> / <a href="https://github.com/gabeur/mmt">code, models, data</a> / <a href="bib/Gabeur2020MMT.txt">bibtex</a>
                  <br>
                  Cross-modal architecture to encode language captions and videos in a common embedding space.
		  </p>
                </td>
              </tr>

              <tr>
                <th style="padding:20px;width:25%;vertical-align:middle";text-align: center>
                  <img src="images/video-pent-logo.svg" alt="video-pent" width="120" height=auto class="center">
                </th>
                <td width="75%" valign="middle">
		  <p>
                  <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/technical-reports/MMT.pdf">
                    <papertitle>CVPR 2020 Video Pentathlon Challenge: Multi-modal Transformer for Video Retrieval</papertitle>
                  </a>
                  <br>
                  <b>Valentin Gabeur</b>, Chen Sun, Karteek Alahari, Cordelia Schmid
                  <br>
                  <em>CVPR Video Pentathlon Workshop, 2020</em> <b> (First place)</b> &nbsp 
                  <br>
	          <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/technical-reports/MMT.pdf">report</a> /
	          <a href="https://arxiv.org/pdf/2008.00744.pdf">paper</a> /
	          <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
	          <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
                  <br>
                  Winning approach for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">CVPR 2020 Video Pentathlon Challenge</a>, a video retrieval competition.
		  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Moulding.png" alt="moulding" width="160" height=auto>
                </td>
                <td width="75%" valign="middle">
		  <p>
                  <a href="https://arxiv.org/abs/1908.00439">
                    <papertitle>Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images</papertitle>
                  </a>
                  <br>
                  <b>Valentin Gabeur</b>, Jean-Sebastien Franco, Xavier Martin, Cordelia Schmid, Gregory Rogez
                  <br>
                  <em>ICCV, 2019</em> &nbsp 
                  <br>
                  <a href="https://arxiv.org/abs/1908.00439">arXiv</a> / <a href="bib/Gabeur2019Moulding.txt">bibtex</a>
                  <br>
                  Efficient 3D shape representation through the combination of depth maps.
		  </p>
                </td>
              </tr>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        Credits to <a href="https://jonbarron.info/">Jon Barron</a> for the template.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
              </td>
              </tr>
          </table>
  </body>
</html>
